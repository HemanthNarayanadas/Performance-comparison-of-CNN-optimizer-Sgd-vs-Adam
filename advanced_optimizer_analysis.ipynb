{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXM0g2xNGkbIBBc3WBgLTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemanthNarayanadas/Performance-comparison-of-CNN-optimizer-Sgd-vs-Adam/blob/main/advanced_optimizer_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee8N5fvk_bkJ",
        "outputId": "a18f7101-098a-4e11-a8a0-95291d9380c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADVANCED CNN OPTIMIZER ANALYSIS\n",
            "============================================================\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Extended Optimizer Comparison:\n",
            "Testing SGD, Adam, RMSprop, and AdamW optimizers...\n",
            "\n",
            "Training with SGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Advanced CNN Optimizer Analysis\n",
        "Extended analysis with additional optimizers and metrics\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ADVANCED CNN OPTIMIZER ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load data (reusing from main analysis)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def create_cnn_model():\n",
        "    \"\"\"Create CNN model for comparison\"\"\"\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Dropout(0.25),\n",
        "\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Dropout(0.25),\n",
        "\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(512, activation='relu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Extended optimizer comparison\n",
        "optimizers_extended = {\n",
        "    'SGD': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
        "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
        "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "    'AdamW': keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01)\n",
        "}\n",
        "\n",
        "print(\"Extended Optimizer Comparison:\")\n",
        "print(\"Testing SGD, Adam, RMSprop, and AdamW optimizers...\")\n",
        "print()\n",
        "\n",
        "results_extended = {}\n",
        "EPOCHS = 10  # Reduced for demonstration\n",
        "\n",
        "for optimizer_name, optimizer in optimizers_extended.items():\n",
        "    print(f\"Training with {optimizer_name}...\")\n",
        "\n",
        "    model = create_cnn_model()\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(x_train, y_train, epochs=EPOCHS, validation_split=0.2,\n",
        "                       batch_size=32, verbose=0)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    results_extended[optimizer_name] = {\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_loss': test_loss,\n",
        "        'training_time': training_time,\n",
        "        'history': history.history\n",
        "    }\n",
        "\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Training Time: {training_time:.2f}s\")\n",
        "    print()\n",
        "\n",
        "# Visualization of extended results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Test accuracy comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "optimizers = list(results_extended.keys())\n",
        "accuracies = [results_extended[opt]['test_accuracy'] for opt in optimizers]\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "bars = plt.bar(optimizers, accuracies, color=colors)\n",
        "plt.title('Test Accuracy Comparison (Extended)', fontweight='bold')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "times = [results_extended[opt]['training_time'] for opt in optimizers]\n",
        "bars = plt.bar(optimizers, times, color=colors)\n",
        "plt.title('Training Time Comparison (Extended)', fontweight='bold')\n",
        "plt.ylabel('Training Time (seconds)')\n",
        "for bar, time_val in zip(bars, times):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "             f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Loss curves\n",
        "plt.subplot(2, 2, 3)\n",
        "for optimizer_name, result in results_extended.items():\n",
        "    plt.plot(result['history']['loss'], label=optimizer_name, linewidth=2)\n",
        "plt.title('Training Loss Curves', fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "plt.subplot(2, 2, 4)\n",
        "for optimizer_name, result in results_extended.items():\n",
        "    plt.plot(result['history']['accuracy'], label=optimizer_name, linewidth=2)\n",
        "plt.title('Training Accuracy Curves', fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of extended analysis\n",
        "print(\"EXTENDED ANALYSIS SUMMARY:\")\n",
        "print(\"=\" * 40)\n",
        "best_accuracy = max(results_extended.items(), key=lambda x: x[1]['test_accuracy'])\n",
        "fastest_training = min(results_extended.items(), key=lambda x: x[1]['training_time'])\n",
        "\n",
        "print(f\"Best Accuracy: {best_accuracy[0]} ({best_accuracy[1]['test_accuracy']:.4f})\")\n",
        "print(f\"Fastest Training: {fastest_training[0]} ({fastest_training[1]['training_time']:.2f}s)\")\n",
        "print()\n",
        "\n",
        "print(\"OPTIMIZER CHARACTERISTICS:\")\n",
        "print(\"• SGD: Simple, memory efficient, requires careful tuning\")\n",
        "print(\"• Adam: Adaptive, generally good performance, more memory usage\")\n",
        "print(\"• RMSprop: Good for RNNs, adaptive learning rate\")\n",
        "print(\"• AdamW: Adam with weight decay, often better generalization\")\n"
      ]
    }
  ]
}