ðŸ§  CNN Optimizer Performance Comparison: SGD vs Adam

A comprehensive analysis comparing SGD (Stochastic Gradient Descent) and Adam (Adaptive Moment Estimation) optimizers on Convolutional Neural Networks (CNNs) for image classification.
This implementation supports any dataset â€” plug in your own image data, and the model automatically adapts.

ðŸ“˜ Project Overview

This project performs a detailed performance comparison between SGD and Adam optimizers using a unified CNN model.
It measures and visualizes the differences in:

Training and validation accuracy

Loss convergence

Speed and computational efficiency

Overall model stability

ðŸ—ï¸ Project Structure
---
cnn_optimizer_project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ cnn_optimizer_comparison.py     # Main CNN optimizer comparison code
â”‚   â””â”€â”€ data_loader.py                  # Dataset loading and preprocessing
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ accuracy_loss_plots.png         # Accuracy/loss plots saved automatically
â”‚   â””â”€â”€ summary_results.csv             # CSV summary of optimizer performance
â””â”€â”€ README.md                           # Project documentation

âš™ï¸ Key Features

âœ… Dataset Flexibility â€“ Works with any dataset (custom or built-in)
âœ… Dual Optimizer Evaluation â€“ CNN trained with both SGD and Adam
âœ… Automatic Performance Metrics â€“ Accuracy, loss, and training time comparison
âœ… Beautiful Visualizations â€“ Accuracy/loss curves and comparison charts
âœ… Customizable Parameters â€“ Easily modify epochs, learning rate, or model depth

ðŸš€ How to Run
ðŸ”¹ 1. Clone or Download
git clone https://github.com/yourusername/cnn-optimizer-comparison.git
cd cnn-optimizer-comparison

ðŸ”¹ 2. Install Requirements
pip install tensorflow numpy pandas matplotlib seaborn scikit-learn

ðŸ”¹ 3. Run the Comparison
python cnn_optimizer_comparison.py

ðŸ“Š Key Findings
Metric	SGD	Adam	Conclusion
Accuracy	Moderate	Higher	Adam performs better overall
Convergence	Slower	Faster	Adam converges quickly
Stability	Requires tuning	More stable	Adam is smoother
Memory Usage	Low	Slightly higher	SGD is lighter
ðŸ§© Model & Training Configuration

Architecture:
Custom CNN with Conv2D â†’ BatchNorm â†’ MaxPooling â†’ Dropout â†’ Dense â†’ Softmax

Input: Automatically resized images

Epochs: 5 (can be increased)

Batch Size: 32

Loss Function: Categorical Crossentropy

Metrics: Accuracy

âš™ï¸ Optimizer Parameters

SGD: learning_rate=0.01, momentum=0.9

Adam: learning_rate=0.001

ðŸ“ˆ Visualizations

ðŸŸ© Training vs Validation Accuracy/Loss Curves
ðŸ“Š Optimizer Comparison Bar Charts
ðŸ§¾ Final Summary Table (Accuracy, Loss, Time)

(Outputs are automatically saved in /results folder)

ðŸŽ“ Educational Value

This project helps you learn:

The difference in behavior between SGD and Adam

How optimizer choice affects CNN training and convergence

How to build a comparative ML experiment

How to visualize and interpret model results

ðŸ”® Future Enhancements

âœ¨ Add more optimizers (RMSprop, AdamW, AdaGrad)
âœ¨ Implement learning rate schedulers
âœ¨ Add transfer learning for advanced comparison
âœ¨ Extend to larger datasets (ImageNet, TinyImageNet, etc.)
âœ¨ Automate hyperparameter sensitivity testing

ðŸ§° Dependencies
TensorFlow >= 2.x  
NumPy  
Pandas  
Matplotlib  
Seaborn  
Scikit-learn


Install all dependencies:

pip install -r requirements.txt

ðŸ“„ License

This project is open-source and available under the MIT License.


